{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58144211",
   "metadata": {},
   "source": [
    "# 텍스트 감정 분석 실습\n",
    "\n",
    "- 텍스트 감정분석의 유용성\n",
    "- 텍스트 데이터를 통해 얻을 수 있는 정보와 활용에 대해 살펴봅니다.\n",
    "- 텍스트 데이터의 특징\n",
    "- 텍스트 데이터의 특징 (1) 텍스트를 숫자로 표현하는 방법\n",
    "- 텍스트 데이터의 특징 (2) Embedding 레이어의 등장\n",
    "- 시퀀스 데이터를 다루는 RNN\n",
    "- 시퀀스 데이터와 RNN의 구조에 대해 알아봅니다.\n",
    "- 꼭 RNN이어야 할까?\n",
    "- 실습과 함께 RNN의 효율성에 대해 살펴봅니다.\n",
    "- IMDB 영화리뷰 감성분석 (1) IMDB 데이터셋 분석\n",
    "- IMDB 영화리뷰 감성분석 (2) 딥러닝 모델 설계와 훈련\n",
    "- IMDB 영화리뷰 감성분석 (3) Word2Vec의 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34b45e9",
   "metadata": {},
   "source": [
    "## 중점으로 생각해볼 것\n",
    "\n",
    "- 텍스트를 어떻게 숫자 행렬로 표현할 수 있을까\n",
    "- 텍스트에는 순서가 중요한데, 입력 데이터의 순서를 인공지능 모델에 어떻게 반영할까"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3dee6e",
   "metadata": {},
   "source": [
    "# 1. 텍스트를 숫자로 표현해보자 : 딕셔너리를 활용(단어사전)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b3b6f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'feel', 'hungry']\n"
     ]
    }
   ],
   "source": [
    " # 처리해야 할 문장을 파이썬 리스트에 옮겨 담았습니다.\n",
    "sentences=['i feel hungry', 'i eat lunch', 'now i feel happy']\n",
    "\n",
    "# 파이썬 split() 메소드를 이용해 단어 단위로 문장을 쪼개 봅니다.\n",
    "word_list = 'i feel hungry'.split()\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a253f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<PAD>', 1: '<BOS>', 2: '<UNK>', 3: 'i', 4: 'feel', 5: 'hungry', 6: 'eat', 7: 'lunch', 8: 'now', 9: 'happy'}\n"
     ]
    }
   ],
   "source": [
    "index_to_word={}  # 빈 딕셔너리를 만들어서\n",
    "\n",
    "# 단어들을 하나씩 채워 봅니다. 채우는 순서는 일단 임의로 하였습니다. 그러나 사실 순서는 중요하지 않습니다. \n",
    "# <BOS>, <PAD>, <UNK>는 관례적으로 딕셔너리 맨 앞에 넣어줍니다. \n",
    "index_to_word[0]='<PAD>'  # 패딩용 단어\n",
    "index_to_word[1]='<BOS>'  # 문장의 시작지점\n",
    "index_to_word[2]='<UNK>'  # 사전에 없는(Unknown) 단어\n",
    "index_to_word[3]='i'\n",
    "index_to_word[4]='feel'\n",
    "index_to_word[5]='hungry'\n",
    "index_to_word[6]='eat'\n",
    "index_to_word[7]='lunch'\n",
    "index_to_word[8]='now'\n",
    "index_to_word[9]='happy'\n",
    "\n",
    "print(index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15971758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딕셔너리의 키 - value 교환\n",
    "\n",
    "word_to_index={word:index for index, word in index_to_word.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0f8e41",
   "metadata": {},
   "source": [
    "- 단어 인코딩: 1개 문장 + 여러개문장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "761563b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트로 변환해 주는 함수를 만들어 봅시다.\n",
    "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. -> 단어사전에 없는 것은 2로 간주.\n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "print(get_encoded_sentence('i eat lunch', word_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f41cb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]]\n"
     ]
    }
   ],
   "source": [
    "# 여러 개의 문장 리스트를 한꺼번에 숫자 텐서로 encode해 주는 함수입니다. \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# sentences=['i feel hungry', 'i eat lunch', 'now i feel happy'] 가 아래와 같이 변환됩니다. \n",
    "encoded_sentences = get_encoded_sentences(sentences, word_to_index)\n",
    "print(encoded_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2cb5a0",
   "metadata": {},
   "source": [
    "- 단어 디코딩: 1개문장 vs 여러문장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42170d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i feel hungry\n"
     ]
    }
   ],
   "source": [
    "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "print(get_decoded_sentence([1, 3, 4, 5], index_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4b67aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i feel hungry', 'i eat lunch', 'now i feel happy']\n"
     ]
    }
   ],
   "source": [
    "# 여러 개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]\n",
    "\n",
    "# encoded_sentences=[[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]] 가 아래와 같이 변환됩니다.\n",
    "print(get_decoded_sentences(encoded_sentences, index_to_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c346f318",
   "metadata": {},
   "source": [
    "## 단어 사전은 입력으로는 사용가능하나 의미를 담기에는 충분하지 않다. 따라서 단어의 의미를 나타내는 벡터를 딥러닝을 통해 학습한다! -> embedding 레이어를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41fcf92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-4.9334228e-02 -3.6393654e-02 -3.6757160e-02  1.9883487e-02]\n",
      "  [ 3.6218848e-02 -4.0630326e-03 -4.5500923e-02 -2.4066186e-02]\n",
      "  [ 2.7092900e-02  1.2248136e-02 -4.2742457e-02  4.5592848e-02]\n",
      "  [-5.7365671e-03  4.2114627e-02  3.9302003e-02 -8.7181479e-04]\n",
      "  [ 3.2004919e-02 -5.2216053e-03  3.3536915e-02  1.6364243e-02]]\n",
      "\n",
      " [[-4.9334228e-02 -3.6393654e-02 -3.6757160e-02  1.9883487e-02]\n",
      "  [ 3.6218848e-02 -4.0630326e-03 -4.5500923e-02 -2.4066186e-02]\n",
      "  [ 4.0693749e-02 -3.5535544e-05  6.1062947e-03  3.0981984e-02]\n",
      "  [-2.8367579e-02 -4.2947270e-02  3.7327562e-02 -6.5178052e-03]\n",
      "  [ 3.2004919e-02 -5.2216053e-03  3.3536915e-02  1.6364243e-02]]\n",
      "\n",
      " [[-4.9334228e-02 -3.6393654e-02 -3.6757160e-02  1.9883487e-02]\n",
      "  [-3.6410652e-02  6.5606125e-03  3.7002992e-02 -2.9701769e-02]\n",
      "  [ 3.6218848e-02 -4.0630326e-03 -4.5500923e-02 -2.4066186e-02]\n",
      "  [ 2.7092900e-02  1.2248136e-02 -4.2742457e-02  4.5592848e-02]\n",
      "  [-9.5931776e-03 -3.7205316e-02  8.2901828e-03  1.9902539e-02]]], shape=(3, 5, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "vocab_size = len(word_to_index)  # 위 예시에서 딕셔너리에 포함된 단어 개수는 10\n",
    "word_vector_dim = 4    # 위 그림과 같이 4차원의 워드 벡터를 가정합니다. \n",
    "\n",
    "embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=word_vector_dim, mask_zero=True)\n",
    "\n",
    "# 숫자로 변환된 텍스트 데이터 [[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]] 에 Embedding 레이어를 적용합니다. \n",
    "# 인풋이될 텍스트 데이터의 벡터길이는 일정해야한다 -> keras.preprocessing함수 사용(<pad>를 추가하면 길이가 5로 일정해진다)\n",
    "raw_inputs = np.array(get_encoded_sentences(sentences, word_to_index), dtype='object')\n",
    "raw_inputs = tf.keras.preprocessing.sequence.pad_sequences(raw_inputs,\n",
    "                                                           value=word_to_index['<PAD>'],\n",
    "                                                           padding='post',\n",
    "                                                           maxlen=5)\n",
    "output = embedding(raw_inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f484d1f",
   "metadata": {},
   "source": [
    "## 지금까지는 텍스트 -> 단어사전 -> 임베딩 벡터\n",
    "## 결국 시퀀스 데이터를 잘 다루는 rnn기반의 모델에 학습시키자!\n",
    "\n",
    "- vanilla rnn: 되먹임 구조만 기억하자. 이전 노드의 출력이 다음노드의 새로운 입력과 같이 입력으로 들어온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a17a98c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 4)           40        \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 8)                 416       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 537\n",
      "Trainable params: 537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 4  # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다. \n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.LSTM(8))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경 가능)\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fd02382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 4)           40        \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 16)          464       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 2,457\n",
      "Trainable params: 2,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# conv1d 활용 -> 문장 전체를 한번에 7짜리 필터로 스캐닝하면서 7단어 이내의 특징을 추출 -> 이것으로 문장을 분류한다.\n",
    "# 모델 예시\n",
    "\n",
    "vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 4   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다. \n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(5))\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67395c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 4)           40        \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 89\n",
      "Trainable params: 89\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#globalmaxpooling1d를 활용 \n",
    "\n",
    "vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 4   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다. \n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadeae8e",
   "metadata": {},
   "source": [
    "# imdb 영화리뷰 감성분석 실습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e14ce700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 0s 0us/step\n",
      "17473536/17464789 [==============================] - 0s 0us/step\n",
      "훈련 샘플 개수: 25000, 테스트 개수: 25000\n"
     ]
    }
   ],
   "source": [
    "imdb = tf.keras.datasets.imdb\n",
    "\n",
    "# IMDb 데이터셋 다운로드 \n",
    "# 여기서 num_words는 몇 개의 단어사전을 만들 것인지 지정해주는것.\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
    "print(f\"훈련 샘플 개수: {len(x_train)}, 테스트 개수: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c74c3d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "라벨:  1\n",
      "1번째 리뷰 문장 길이:  218\n",
      "2번째 리뷰 문장 길이:  189\n"
     ]
    }
   ],
   "source": [
    "#실제 예시 살펴보기: 숫자로 인코딩된 문장!\n",
    "\n",
    "print(x_train[0])  # 1번째 리뷰데이터\n",
    "print('라벨: ', y_train[0])  # 1번째 리뷰데이터의 라벨\n",
    "print('1번째 리뷰 문장 길이: ', len(x_train[0]))\n",
    "print('2번째 리뷰 문장 길이: ', len(x_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d033c8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#imdb데이터에 encoding된 딕셔너리까지 제공된다. get_word_index()\n",
    "# the: 1 와 같은 형태로 구성된 딕셔너리\n",
    "\n",
    "word_to_index = imdb.get_word_index()\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "print(index_to_word[1])     # 'the' 가 출력됩니다. \n",
    "print(word_to_index['the'])  # 1 이 출력됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf2caea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as you with out themselves powerful lets loves their becomes reaching had journalist of lot from anyone to have after out atmosphere never more room and it so heart shows to years of every never going and help moments or of every chest visual movie except her was several of enough more with is now current film as you of mine potentially unfortunately of you than him that with out themselves her get for was camp of you movie sometimes movie that with scary but and to story wonderful that in seeing in character to of 70s musicians with heart had shadows they of here that with her serious to have does when from why what have critics they is you that isn't one will very to as itself with other and in of seen over landed for anyone of and br show's to whether from than out themselves history he name half some br of and odd was two most of mean for 1 any an boat she he should is thought frog but of script you not while history he heart to real at barrel but when from one bit then have two of script their with her nobody most that with wasn't to with armed acting watch an for with heartfelt film want an\n"
     ]
    }
   ],
   "source": [
    "# 보정 전 x_train[0] 데이터\n",
    "print(get_decoded_sentence(x_train[0], index_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "183da41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS>\n",
      "4\n",
      "the\n",
      "this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "# 보정하기: word_to_index는 단어 출현 빈도 기준으로 내림차수 정렬되어 있다.\n",
    "#실제 인코딩 인덱스는 제공된 word_to_index에서 index 기준으로 3씩 뒤로 밀려 있습니다.  \n",
    "word_to_index = {k:(v+3) for k,v in word_to_index.items()}\n",
    "\n",
    "# 처음 몇 개 인덱스는 사전에 정의되어 있습니다.\n",
    "word_to_index[\"<PAD>\"] = 0\n",
    "word_to_index[\"<BOS>\"] = 1\n",
    "word_to_index[\"<UNK>\"] = 2  # unknown\n",
    "word_to_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "\n",
    "print(index_to_word[1])     # '<BOS>' 가 출력됩니다. \n",
    "print(word_to_index['the'])  # 4 이 출력됩니다. \n",
    "print(index_to_word[4])     # 'the' 가 출력됩니다.\n",
    "\n",
    "# 보정 후 x_train[0] 데이터\n",
    "print(get_decoded_sentence(x_train[0], index_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7aebb735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력데이터: this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n",
      "라벨:  1\n"
     ]
    }
   ],
   "source": [
    "print('입력데이터:',get_decoded_sentence(x_train[0], index_to_word))\n",
    "print('라벨: ', y_train[0])  # 1번째 리뷰데이터의 라벨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26296f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  234.75892\n",
      "문장길이 최대 :  2494\n",
      "문장길이 표준편차 :  172.91149458735703\n",
      "pad_sequences maxlen :  580\n",
      "전체 문장의 0.94536%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    }
   ],
   "source": [
    "#pad_sequence로 문장의 길이를 통일시키자!\n",
    "\n",
    "total_data_text = list(x_train) + list(x_test)\n",
    "# 텍스트데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print(f'전체 문장의 {np.sum(num_tokens < max_tokens) / len(num_tokens)}%가 maxlen 설정값 이내에 포함됩니다. ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f869a9",
   "metadata": {},
   "source": [
    "**post(문장뒤에), pre(앞에) 패딩을 어디에 하냐에 따라 성능이 달라질 수 있다.**\n",
    "\n",
    "-> 두가지 방식의 결과를 비교해볼것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d858d22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 580)\n"
     ]
    }
   ],
   "source": [
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        value=word_to_index[\"<PAD>\"],\n",
    "                                                        padding='pre', #'post' 혹은 'pre'\n",
    "                                                        maxlen=maxlen)\n",
    "\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='pre', # 'post' 혹은 'pre'\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "461cacf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 8)                 800       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 160,881\n",
      "Trainable params: 160,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#모델링\n",
    "vocab_size = 10000\n",
    "word_vector_dim =16\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.LSTM(8))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경 가능)\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a796da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 580)\n",
      "(15000,)\n"
     ]
    }
   ],
   "source": [
    "#직접 train/val dataset을 분리하기: 전체 train의 몇 %를 val로 두어야 효율적일까?\n",
    "\n",
    "# validation set 10000건 분리\n",
    "x_val = x_train[:10000]   \n",
    "y_val = y_train[:10000]\n",
    "\n",
    "# validation set을 제외한 나머지 15000건\n",
    "partial_x_train = x_train[10000:]  \n",
    "partial_y_train = y_train[10000:]\n",
    "\n",
    "print(partial_x_train.shape)\n",
    "print(partial_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e3fa6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "30/30 [==============================] - 11s 47ms/step - loss: 0.6932 - accuracy: 0.5092 - val_loss: 0.6933 - val_accuracy: 0.5011\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.6929 - accuracy: 0.5198 - val_loss: 0.6933 - val_accuracy: 0.5026\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.6926 - accuracy: 0.5233 - val_loss: 0.6930 - val_accuracy: 0.5032\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.6920 - accuracy: 0.5267 - val_loss: 0.6926 - val_accuracy: 0.5053\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.6907 - accuracy: 0.5281 - val_loss: 0.6920 - val_accuracy: 0.5063\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.6864 - accuracy: 0.5206 - val_loss: 0.6827 - val_accuracy: 0.5247\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.6927 - accuracy: 0.5232 - val_loss: 0.6928 - val_accuracy: 0.5068\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.6890 - accuracy: 0.5159 - val_loss: 0.6915 - val_accuracy: 0.5056\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.6848 - accuracy: 0.5239 - val_loss: 0.6892 - val_accuracy: 0.5165\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.6800 - accuracy: 0.5312 - val_loss: 0.6880 - val_accuracy: 0.5105\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.6768 - accuracy: 0.5348 - val_loss: 0.6865 - val_accuracy: 0.5190\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.6735 - accuracy: 0.5268 - val_loss: 0.6850 - val_accuracy: 0.5215\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.6704 - accuracy: 0.5282 - val_loss: 0.6845 - val_accuracy: 0.5141\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.6670 - accuracy: 0.5302 - val_loss: 0.6827 - val_accuracy: 0.5149\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.6640 - accuracy: 0.5375 - val_loss: 0.6814 - val_accuracy: 0.5256\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.6613 - accuracy: 0.5364 - val_loss: 0.6820 - val_accuracy: 0.5244\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.6591 - accuracy: 0.5376 - val_loss: 0.6828 - val_accuracy: 0.5193\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.6562 - accuracy: 0.5375 - val_loss: 0.6805 - val_accuracy: 0.5337\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.6538 - accuracy: 0.5389 - val_loss: 0.6799 - val_accuracy: 0.5245\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.6213 - accuracy: 0.6534 - val_loss: 0.5487 - val_accuracy: 0.7532\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꿔보기. \n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bee3efef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 8s - loss: 0.5602 - accuracy: 0.7433\n",
      "[0.5602283477783203, 0.7433199882507324]\n"
     ]
    }
   ],
   "source": [
    "#모델 평가\n",
    "\n",
    "results = model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09eea5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    }
   ],
   "source": [
    "# fit()을 하면 history변수에 각 에포크 마다의 항목들이 저장되어있다.\n",
    "\n",
    "history_dict = history.history\n",
    "print(history_dict.keys()) # epoch에 따른 그래프를 그려볼 수 있는 항목들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b2bc9c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e418e09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # 그림을 초기화\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35488f3e",
   "metadata": {},
   "source": [
    "## word2vec을 적용해보자: 워드임베딩 효과를 증대 = 임베딩 레이어에 학습된 워드 벡터들이 유의미하게 공간상에 표현되었는가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d4f4bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = model.layers[0]\n",
    "weights = embedding_layer.get_weights()[0]\n",
    "print(weights.shape)    # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dae79439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습한 Embedding 파라미터를 파일에 써서 저장합니다. \n",
    "word2vec_file_path = os.getenv('HOME')+'/aiffel/workspace/sentimental_classification/data/word2vec.txt'\n",
    "f = open(word2vec_file_path, 'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-4, word_vector_dim))  # 몇개의 벡터를 얼마 사이즈로 기재할지 타이틀을 씁니다.\n",
    "\n",
    "# 단어 개수(에서 특수문자 4개는 제외하고)만큼의 워드 벡터를 파일에 기록합니다. \n",
    "vectors = model.get_weights()[0]\n",
    "for i in range(4,vocab_size):\n",
    "    f.write('{} {}\\n'.format(index_to_word[i], ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b7a6cf6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.15070055, -0.06290054, -0.03158965, -0.08576063, -0.11936969,\n",
       "        0.08202324,  0.13164914,  0.07469115, -0.0868268 , -0.07699308,\n",
       "       -0.12023365,  0.07731161, -0.12293761,  0.14352158,  0.09744259,\n",
       "        0.12226139], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gensim패키지 사용 - 임베딩 파라미터를 읽어서 word vector로 활용\n",
    "\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "\n",
    "word_vectors = Word2VecKeyedVectors.load_word2vec_format(word2vec_file_path, binary=False)\n",
    "vector = word_vectors['computer']\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7d85ffea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('alexandra', 0.9188196063041687),\n",
       " ('brutality', 0.9131999015808105),\n",
       " ('dashing', 0.9119632244110107),\n",
       " ('ought', 0.9107054471969604),\n",
       " ('glimpse', 0.9099799394607544),\n",
       " ('slavery', 0.9087499380111694),\n",
       " ('guide', 0.9080654382705688),\n",
       " ('remade', 0.906608521938324),\n",
       " ('dudley', 0.9027790427207947),\n",
       " ('nominated', 0.9027639031410217)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word(\"love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca852df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# google에서 제공하는 word2vec이라는 워드임베딩모델을 가져온다!\n",
    "# 300만 단어를 300차원의 벡터로 표현\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word2vec_path = os.getenv('HOME')+'/aiffel/workspace/sentimental_classification/data/GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=1000000)\n",
    "vector = word2vec['computer']\n",
    "vector.shape    # 무려 300dim의 워드 벡터.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "86e2a1c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loved', 0.6907791495323181),\n",
       " ('adore', 0.6816873550415039),\n",
       " ('loves', 0.661863386631012),\n",
       " ('passion', 0.6100708842277527),\n",
       " ('hate', 0.600395679473877),\n",
       " ('loving', 0.5886635780334473),\n",
       " ('affection', 0.5664337873458862),\n",
       " ('undying_love', 0.5547304749488831),\n",
       " ('absolutely_adore', 0.5536840558052063),\n",
       " ('adores', 0.5440906882286072)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#300dim의 벡터로 이루어진 300만 개의 단어입니다. \n",
    "#이 단어 사전을 메모리에 모두 로딩하면 아주 높은 확률로 여러분의 실습환경에 메모리 에러가 날 것입니다. \n",
    "#그래서 KeyedVectors.load_word2vec_format 메서드로 워드 벡터를 로딩할 때 가장 많이 사용되는 상위 100만 개만 limt으로 조건을 주어 로딩했습니다.\n",
    "# 메모리를 다소 많이 소비하는 작업이니 유의해 주세요.\n",
    "\n",
    "word2vec.similar_by_word(\"love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2e604283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding layer를 word2vec으로 바꿔주기\n",
    "\n",
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 300  # 워드 벡터의 차원수\n",
    "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
    "\n",
    "# embedding_matrix에 Word2Vec 워드 벡터를 단어 하나씩마다 차례차례 카피한다.\n",
    "for i in range(4,vocab_size):\n",
    "    if index_to_word[i] in word2vec:\n",
    "        embedding_matrix[i] = word2vec[index_to_word[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1b41fd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 580, 300)          3000000   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 574, 16)           33616     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 114, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 108, 16)           1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 3,035,569\n",
      "Trainable params: 3,035,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "# 모델 구성\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, \n",
    "                                     word_vector_dim, \n",
    "                                     embeddings_initializer=Constant(embedding_matrix),  # 카피한 임베딩을 여기서 활용\n",
    "                                     input_length=maxlen, \n",
    "                                     trainable=True))   # trainable을 True로 주면 Fine-tuning\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(5))\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid')) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f823c34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "30/30 [==============================] - 24s 187ms/step - loss: 0.6982 - accuracy: 0.5039 - val_loss: 0.6898 - val_accuracy: 0.5398\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 4s 138ms/step - loss: 0.6788 - accuracy: 0.5711 - val_loss: 0.6925 - val_accuracy: 0.5581\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 4s 140ms/step - loss: 0.6419 - accuracy: 0.6421 - val_loss: 0.6259 - val_accuracy: 0.6676\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 4s 139ms/step - loss: 0.5093 - accuracy: 0.7883 - val_loss: 0.4400 - val_accuracy: 0.8184\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 4s 139ms/step - loss: 0.3256 - accuracy: 0.8733 - val_loss: 0.3413 - val_accuracy: 0.8545\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 4s 141ms/step - loss: 0.2185 - accuracy: 0.9197 - val_loss: 0.3322 - val_accuracy: 0.8607\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 4s 141ms/step - loss: 0.1552 - accuracy: 0.9490 - val_loss: 0.3021 - val_accuracy: 0.8765\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 4s 141ms/step - loss: 0.1065 - accuracy: 0.9736 - val_loss: 0.3090 - val_accuracy: 0.8769\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 4s 142ms/step - loss: 0.0743 - accuracy: 0.9853 - val_loss: 0.3316 - val_accuracy: 0.8720\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 4s 142ms/step - loss: 0.0516 - accuracy: 0.9934 - val_loss: 0.3289 - val_accuracy: 0.8766\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 4s 143ms/step - loss: 0.0348 - accuracy: 0.9969 - val_loss: 0.3515 - val_accuracy: 0.8746\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 4s 143ms/step - loss: 0.0244 - accuracy: 0.9983 - val_loss: 0.3533 - val_accuracy: 0.8772\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 4s 144ms/step - loss: 0.0181 - accuracy: 0.9989 - val_loss: 0.3656 - val_accuracy: 0.8766\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 4s 144ms/step - loss: 0.0146 - accuracy: 0.9990 - val_loss: 0.3775 - val_accuracy: 0.8770\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 4s 143ms/step - loss: 0.0111 - accuracy: 0.9991 - val_loss: 0.3894 - val_accuracy: 0.8776\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 4s 144ms/step - loss: 0.0089 - accuracy: 0.9994 - val_loss: 0.3996 - val_accuracy: 0.8771\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 4s 145ms/step - loss: 0.0071 - accuracy: 0.9995 - val_loss: 0.4108 - val_accuracy: 0.8773\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 4s 145ms/step - loss: 0.0060 - accuracy: 0.9997 - val_loss: 0.4181 - val_accuracy: 0.8776\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 4s 146ms/step - loss: 0.0051 - accuracy: 0.9997 - val_loss: 0.4407 - val_accuracy: 0.8739\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 4s 143ms/step - loss: 0.0040 - accuracy: 0.9998 - val_loss: 0.4383 - val_accuracy: 0.8746\n"
     ]
    }
   ],
   "source": [
    "# 학습의 진행\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6cc42e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 2s - loss: 0.4732 - accuracy: 0.8649\n",
      "[0.4732283651828766, 0.8649200201034546]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
